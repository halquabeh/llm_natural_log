{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f967ea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7657fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bhaskar_mukhoty/.local/bin/jupyter\n"
     ]
    }
   ],
   "source": [
    "!which jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "757a67ca",
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.8.10"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from config.config import Config\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m login\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# from config.config import Config\n",
    "import torch\n",
    "from models.load_model import Model\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "# fix the seeds\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# model_name = \"EleutherAI/pythia-2.8b\"\n",
    "# model_name = \"openai-community/gpt2-large\"\n",
    "# model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "# model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Natural Log Model Parameters')\n",
    "parser.add_argument('--transform', type=str, default=\"PCA\", help='Transformation method')\n",
    "parser.add_argument('--Tdim', type=int, default=1, help='Transform dimension')\n",
    "parser.add_argument('--k', type=int, default=30, help='Number of samples')\n",
    "parser.add_argument('--num_examples', type=int, default=3, help='Number of examples')\n",
    "parser.add_argument('--upper_bound', type=int, default=100, help='Upper bound for random numbers')\n",
    "parser.add_argument('--context', type=str, default='same', help='Context type')\n",
    "parser.add_argument('--data', type=str, default='numerics', help='Data type')\n",
    "parser.add_argument('--groups', nargs='+', type=int, default=[1,2,3,4], help='Group sizes')\n",
    "parser.add_argument('--save', action='store_true', default=True, help='Save results')\n",
    "parser.add_argument('--plot', action='store_true', default=True, help='Plot results')\n",
    "parser.add_argument('--model_name', type=str, default=\"meta-llama/Llama-3.1-8B\", help='Model name')\n",
    "parser.add_argument('--device', type=str, default=\"2\", help='Device to run on')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "interval_function = lambda i: range(10**i - 2 * 10, 10**i + 2 * 10) if i >1 else range(1, 40) \n",
    "\n",
    "model_name = args.model_name\n",
    "device = torch.device(f\"cuda:{args.device}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Model(model_name, device)\n",
    "\n",
    "# Load the prompts data\n",
    "from utils.data_utils import generate_prompts_numerals,generate_prompts_symbols\n",
    "if args.data.lower() == 'numerics':\n",
    "    prompts = generate_prompts_numerals(args.k, args.num_examples, args.upper_bound, args.groups,interval_function , context=args.context)\n",
    "else:\n",
    "    prompts = generate_prompts_symbols(args.k, args.num_examples, args.groups , context=args.context)\n",
    "\n",
    "from utils.data_utils import strings_to_numbers \n",
    "# Initialize a dictionary to save all layers' hidden states raw imported from models layers\n",
    "results = {} # the shape of this dictionary, will be hacing for every layer two dict hidden_states and answers. and inside each of these they woll be groups\n",
    "# Iterate through each layer\n",
    "for idx, layer in enumerate(list(range(0, model.model.config.num_hidden_layers, 1))):\n",
    "    hidden_states = {}\n",
    "    answers = {} # assuming the model answer the last number\n",
    "    # Collect hidden states for each prompt in each group\n",
    "    for key, prompt_list in prompts.items():\n",
    "        hidden_states[key] = []\n",
    "        answers[key] = []\n",
    "        for prompt in prompt_list:\n",
    "            hidden_state = model.get_hidden_state(prompt, layer_index=layer)\n",
    "            hidden_states[key].append(hidden_state)\n",
    "            last_number = prompt[prompt.rfind(',')+1:prompt.rfind('=')]\n",
    "            # Assert that model can generate the correct answer for this prompt\n",
    "            # generated = model.predict(prompt)\n",
    "            # assert last_number in generated, f\"Model failed to generate correct answer {last_number} in output {generated}\"\n",
    "            answers[key].append(last_number)\n",
    "        if args.data.lower() == 'symbols':\n",
    "            answers[key] = strings_to_numbers(answers[key])\n",
    "    # Save the hidden states and answers for the current layer\n",
    "    results[layer] = {'hidden_states': hidden_states, 'answers': answers}\n",
    "\n",
    "\n",
    "# Apply T(x) using PLS or PCA\n",
    "from utils.compute_utils import transform_hidden_states,analyze_transformed_hidden_states\n",
    "results_T = transform_hidden_states(results,args.transform,args.Tdim)\n",
    "\n",
    "# Coompute the metris for sublineraity and monotonicity:results_T\n",
    "results_analysis = analyze_transformed_hidden_states(results_T)\n",
    "\n",
    "from utils.visual_utils import plot_pca_projections\n",
    "import os\n",
    "# save_path = os.path.join('checkpoints', f'{model_name.replace(\"/\", \"_\")}_{args.data}_{args.transfom}_{args.Tdim}_{args.num_examples}_{args.k}')\n",
    "# plot_pca_projections(results_analysis, save_path)\n",
    "\n",
    "\n",
    "save_path = os.path.join('checkpoints', f'{model_name.replace(\"/\", \"_\")}_{args.data}_{args.transform}_{args.Tdim}_{args.num_examples}_{args.k}_R4.pth')\n",
    "torch.save(results_analysis, save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
